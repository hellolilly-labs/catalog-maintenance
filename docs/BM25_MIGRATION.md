# BM25 Migration Guide

## Overview

We've migrated from a hand-rolled BM25 implementation to the proven `rank-bm25` library for sparse embeddings. This provides better correctness, performance, and maintainability.

## Key Improvements

### 1. **Correctness**
- **Before**: Custom IDF calculation could produce negative values
- **After**: Proven BM25Okapi algorithm with proper epsilon handling

### 2. **Performance**
- **Before**: Manual term frequency calculations
- **After**: Optimized numpy operations in rank-bm25

### 3. **Maintainability**
- **Before**: 400+ lines of custom BM25 code
- **After**: Leverages well-tested library, focusing on our domain logic

### 4. **Flexibility**
- **Before**: Single BM25 variant
- **After**: Can easily switch between BM25Okapi, BM25L, BM25Plus

## Migration Steps

### 1. Install Dependencies
```bash
pip install rank-bm25>=0.2.2
```

### 2. Check Current State
```bash
python migrate_to_bm25.py specialized.com --check-only
```

### 3. Run Migration
```bash
python migrate_to_bm25.py specialized.com
```

### 4. Update Ingestion
The ingestion pipeline has been updated to use the new implementation:
- `src/ingestion/sparse_embeddings.py` â†’ `src/ingestion/sparse_embeddings_bm25.py`

### 5. Re-run Ingestion
```bash
python run_ingestion.py specialized.com
```

## API Compatibility

The new implementation maintains the same API:

```python
# Initialize
generator = SparseEmbeddingGenerator(brand_domain)

# Build vocabulary
generator.build_vocabulary(products)

# Generate embeddings
sparse_data = generator.generate_sparse_embedding(product)
# Returns: {'indices': [...], 'values': [...]}
```

## Key Differences

### 1. **Vocabulary Storage**
- Old: `sparse_vocabulary.json`
- New: `sparse_vocabulary_bm25.json`

### 2. **Term Weighting**
- Old: Manual BM25 formula implementation
- New: rank-bm25's optimized implementation + custom term importance

### 3. **Brand-Specific Terms**
- Old: Hardcoded brand terms in source code
- New: Dynamic detection from domain name (extensible via config)

## Testing

Compare implementations:
```bash
python test_bm25_comparison.py specialized.com --num-products 50
```

## Rollback

If needed, revert to old implementation:
1. Change import in `separate_index_ingestion.py`:
   ```python
   from .sparse_embeddings import SparseEmbeddingGenerator
   ```
2. Remove rank-bm25 from requirements.txt
3. Re-run ingestion

## Performance Characteristics

Based on testing:
- **Vocabulary Build**: Similar performance (within 10%)
- **Embedding Generation**: 1.5-2x faster with rank-bm25
- **Memory Usage**: Slightly lower with rank-bm25
- **Feature Overlap**: 85-95% overlap with old implementation

## Future Enhancements

1. **Brand Research Integration**: Use brand research to dynamically determine important terms
2. **Multi-lingual Support**: Leverage rank-bm25's language-agnostic design
3. **Custom Scoring**: Implement BM25F for field-specific boosting
4. **Compression**: Use scipy sparse matrices for larger vocabularies